# -*- coding: utf-8 -*-
"""variational_autoencoder.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WXfMnTQYIaMFCMB-0qIxx3FRZgCgN9Az
"""

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
  
  
learning_para =0.001
epochs=30000
batch_size =32



from tensorflow.examples.tutorials.mnist import  input_data
database= input_data.read_data_sets('/content/data',one_hot = True)

"""network parameters"""

image_dimension=784
neural_network_dimension=512
latent_variable_dimension=2

"""initialization"""

def xavier(in_shape):
  val=tf.random_normal(shape=in_shape , stddev=1/tf.sqrt(in_shape[0]/2))
  return(val)

"""weight and bias dictionaries"""

weight= {"weight_matrix_encoder_hidden":tf.Variable(xavier([image_dimension,neural_network_dimension])),
         "weight_mean_hidden":tf.Variable(xavier([neural_network_dimension,latent_variable_dimension])),
          "weight_std_hidden":tf.Variable(xavier([neural_network_dimension,latent_variable_dimension])),
         "weight_matrix_decoder_hidden":tf.Variable(xavier([latent_variable_dimension,neural_network_dimension])),
         "weight_decoder":tf.Variable(xavier([neural_network_dimension,image_dimension]))
         }


bias = {"bias_matrix_encoder_hidden":tf.Variable(xavier([neural_network_dimension])) ,     
        "bias_mean_hidden":tf.Variable(xavier([latent_variable_dimension])),
        "bias_std_hidden":tf.Variable(xavier([latent_variable_dimension])),
        "bias_matrix_decoder_hidden":tf.Variable(xavier([neural_network_dimension])),
        "bias_decoder":tf.Variable(xavier([image_dimension]))
        }

"""encoder_section"""

image_x = tf.placeholder(tf.float32,shape=[None,image_dimension])
encoder_layer=tf.add(tf.matmul(image_x,weight["weight_matrix_encoder_hidden"]),bias["bias_matrix_encoder_hidden"])
encoder_layer=tf.tanh(encoder_layer)


mean_layer=tf.add(tf.matmul(encoder_layer,weight["weight_mean_hidden"]),bias["bias_mean_hidden"])
std_layer=tf.add(tf.matmul(encoder_layer,weight["weight_std_hidden"]),bias["bias_std_hidden"])

"""reparamatrization_trick"""

epsilon=tf.random_normal(tf.shape(std_layer),dtype=tf.float32,mean=0.0 , stddev=1.0)
latent_layer=mean_layer+tf.exp(0.5*std_layer)*epsilon

"""DECODER SECTION"""

decoder_hidden=tf.add(tf.matmul(latent_layer,weight["weight_matrix_decoder_hidden"]),bias["bias_matrix_decoder_hidden"])
decoder_hidden=tf.tanh(decoder_hidden)

decoder_output=tf.add(tf.matmul(decoder_hidden,weight["weight_decoder"]),bias["bias_decoder"])
decoder_output=tf.sigmoid(decoder_output)

"""VARIATIONAL ENCODER LOSS"""

def loss_function(original_image,reconstructed_image):
  data_fidelity_loss=original_image*tf.log(1e-10 + reconstructed_image)+(1-original_image)*tf.log(1e-10 + 1-reconstructed_image)
  data_fidelity_loss=-tf.reduce_sum(data_fidelity_loss,1)

  kl_div_loss=1+std_layer-tf.square(mean_layer)-tf.exp(std_layer)
  kl_div_loss=-0.5*tf.reduce_sum(kl_div_loss,1)
  
  alpha=1
  beta=1
  network_loss=tf.reduce_mean(alpha*data_fidelity_loss+beta*kl_div_loss)
  return(network_loss)

loss_value= loss_function(image_x,decoder_output)
optimizer=tf.train.RMSPropOptimizer(learning_para).minimize(loss_value)

init=tf.global_variables_initializer()

sess=tf.Session()
sess.run(init)
for i in range(epochs):
  x_batch,_ =database.train.next_batch(batch_size)
  _,loss= sess.run([optimizer,loss_value],feed_dict= {image_x : x_batch})
  if i%5000 ==0:
    print("loss is {0} at iteration{1}".format(loss,i))

"""# TESTING"""

noise_x = tf.placeholder(tf.float32,shape=[None,latent_variable_dimension])

  decoder_hidden=tf.add(tf.matmul(noise_x,weight["weight_matrix_decoder_hidden"]),bias["bias_matrix_decoder_hidden"])
decoder_hidden=tf.tanh(decoder_hidden)

decoder_output=tf.add(tf.matmul(decoder_hidden,weight["weight_decoder"]),bias["bias_decoder"])
decoder_output=tf.sigmoid(decoder_output)

"""OUTPUT VISUALIZATION"""

n=20
x_limit =np.linspace(-2,2,n)
y_limit=np.linspace(-2,2,n)

empty_image=np.empty((28*n,28*n))

for i,zi in enumerate(x_limit):
  for j , pi in enumerate(y_limit):
    generated_latent_layer=np.array([[zi,pi]]*batch_size)
    #generated_latent_layer=np.random.normal(0,1,size=[batch_size,latent_variable_dimension] )
    generated_image=sess.run(decoder_output,feed_dict={noise_x : generated_latent_layer})
    empty_image[(n-i-1)*28:(n-i)*28,j*28:(j+1)*28]= generated_image[0].reshape(28,28)
plt.figure(figsize=(8,10))
x,y = np.meshgrid(x_limit,y_limit)
plt.imshow(empty_image,origin="upper",cmap="gray")
plt.grid(False)
plt.show()

x_sample,y_sample =database.test.next_batch(batch_size+15000)
print(x_sample.shape)

interim = sess.run(latent_layer , feed_dict={image_x : x_sample })
print(interim.shape)
colors=np.argmax(y_sample,1)

plt.figure(figsize=(8,6))
plt.scatter(interim[:,0],interim[:,1],c=colors,cmap='viridis')
plt.colorbar()
plt.grid()
sess.close();
